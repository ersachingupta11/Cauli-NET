# -*- coding: utf-8 -*-
"""Hybrid_Model_With_GradCam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zfEh-fQRpDN0-rIHPRlb_NHLhubfnZ0B
"""

model.compile(
 loss='categorical_crossentropy',
 optimizer='adam',
 metrics=['accuracy']
)

history = model.fit([X_train, X_train],Y_train,batch_size=32,validation_data=([X_val, X_val],Y_val),epochs= 2,
                 callbacks=[lr_sc])

plt.figure(1, figsize = (20, 12))
plt.subplot(1,2,1)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.plot( history.history["loss"], label = "Training Loss")
plt.plot( history.history["val_loss"], label = "Validation Loss")
plt.grid(True)
plt.legend()

plt.subplot(1,2,2)
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.plot( history.history["accuracy"], label = "Training Accuracy")
plt.plot( history.history["val_accuracy"], label = "Validation Accuracy")
plt.grid(True)
plt.legend()

pred = model.predict([X_val,X_val])
y_pred = np.argmax(pred, axis=-1)
predictions=to_categorical(y_pred)

# get the classification report
res=model.evaluate([X_train,X_train],Y_train)
re=model.evaluate([X_val,X_val],Y_val)
print(classification_report(Y_val, predictions, target_names=labels))
print("----------------------------------------------------")
print("Training Loss of the model: ",res[0])
print("----------------------------------------------------")
print("Training Accuracy of the model: ",res[1]*100)
print("----------------------------------------------------")
print("Valiadation Loss of the model: ",re[0])
print("----------------------------------------------------")
print("Valiadation Accuracy of the model: ",re[1]*100)

# Determine the number of samples to select
num_samples = int(0.1 * len(X_val))

# Select a random subset of the validation data
X_val_subset = X_val[:num_samples]
Y_val_subset = Y_val[:num_samples]

# Evaluate the model on the subset of validation data
re = model.evaluate([X_val_subset, X_val_subset], Y_val_subset)
print("Test Loss of the model: ",re[0])
print("----------------------------------------------------")
print("Test Accuracy of the model: ",re[1]*100)

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split the validation data into 10% for evaluation
X_val_subset, _, Y_val_subset, _ = train_test_split(X_val, Y_val, test_size=0.1, random_state=42)
#assert X_val_subset.shape == X_val.shape, "Equal Shape"
# Evaluate the model on the subset of validation data
r = model.evaluate([X_val_subset, X_val_subset], Y_val_subset)
print("Test Loss of the model: ",r[0])
print("----------------------------------------------------")
print("Test Accuracy of the model: ",r[1]*100)

Y_true = np.argmax(Y_val, axis=1)
predictions = np.argmax(predictions, axis=1)

import seaborn as sb
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.metrics import multilabel_confusion_matrix,ConfusionMatrixDisplay

conf_matrix = confusion_matrix(Y_true, predictions)
conf_matrix1 = multilabel_confusion_matrix(Y_true, predictions)

# Creating a function to report confusion metrics
def confusion_metrics_stat (conf_matrix):
# save confusion matrix and slice into four pieces
    TP = conf_matrix[0][0]
    TN = conf_matrix[1][1]
    FP = conf_matrix[0][1]
    FN = conf_matrix[1][0]
    print('True Positives:', TP)
    print('True Negatives:', TN)
    print('False Positives:', FP)
    print('False Negatives:', FN)

    # calculate accuracy
    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))

    # calculate mis-classification
    conf_misclassification = 1- conf_accuracy

    # calculate the sensitivity
    conf_sensitivity = (TP / float(TP + FN))
    # calculate the specificity
    conf_specificity = (TN / float(TN + FP))

    # calculate precision
    conf_precision = (TN / float(TN + FP))
    # calculate f_1 score
    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))
    print('-'*50)
    print(f'Accuracy: {round(conf_accuracy,2)}')
    print(f'Mis-Classification: {round(conf_misclassification,2)}')
    print(f'Sensitivity: {round(conf_sensitivity,2)}')
    print(f'Specificity: {round(conf_specificity,2)}')
    print(f'Precision: {round(conf_precision,2)}')
    print(f'f_1 Score: {round(conf_f1,2)}')
    print('-'*50)
    print('-'*50)

for j in range(conf_matrix1.shape[0]):
    current_matrix = conf_matrix1[j]

    #disp = ConfusionMatrixDisplay(current_matrix, display_labels=None)
    #disp.plot(include_values=True, cmap="viridis", ax=None, xticks_rotation="vertical")
    fig,ax = plt.subplots(figsize=(5, 3))
    sb.heatmap(current_matrix, annot=True, linewidths=0.01,cmap="magma",linecolor="gray", fmt= '.1f',ax=ax)
    plt.xlabel("Predicted Class")
    plt.ylabel("True Class")
    plt.title("Confusion Matrix")
    plt.show()
    confusion_metrics_stat (current_matrix)

# plot the confusion matrix
fig,ax = plt.subplots(figsize=(10, 10))
sb.heatmap(conf_matrix, annot=True, linewidths=0.01,cmap="magma",linecolor="gray", fmt= '.1f',ax=ax)
plt.xlabel("Predicted Class")
plt.ylabel("True Class")
plt.title("Confusion Matrix")
ax.set_xticklabels(labels = labels,fontdict=None,rotation=45)
ax.set_yticklabels(labels = labels,fontdict=None,rotation=45)
plt.show()

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(Y_true, predictions)
print("MSE::::: ===>",mse)
#print(classification_report(Y_true, predictions,target_names=labels))
from sklearn.metrics import f1_score
print("f1_score:::::: ====>",f1_score(Y_true, predictions,average="weighted"))

#Testing Data TNSE
import numpy as np
from sklearn.manifold import TSNE

# Load the trained model
#model = tf.keras.models.load_model("/kaggle/working/combine.h5")

# Create a new model without the final dense layer
feature_extractor_model = Model(inputs=model.input, outputs=model.get_layer('global_average_pooling2d_1').output)

# Get the intermediate features for the validation data
X_val_features = feature_extractor_model.predict([X_val,X_val])

# Apply t-SNE on the features
tsne = TSNE(n_components=3, random_state=42)
X_val_tsne = tsne.fit_transform(X_val_features)

# X_val_tsne now contains the t-SNE embeddings for the validation data

